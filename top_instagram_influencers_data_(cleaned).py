# -*- coding: utf-8 -*-
"""Top Instagram Influencers Data (Cleaned).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EbV1FaIyZrDHZemiTdbaU4QO6KHKs-kc
"""

!pip install pandas numpy scikit-learn matplotlib seaborn flask flask-cors mysql-connector-python psycopg2

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('/content/top_insta_influencers_data.csv')

# Convert numerical columns properly
df.replace({'b': 'e9', 'm': 'e6', 'k': 'e3', '%': ''}, regex=True, inplace=True)
numeric_cols = ['total_likes', 'posts', 'followers', 'avg_likes', '60_day_eng_rate', 'new_post_avg_like']
df[numeric_cols] = df[numeric_cols].astype(float)

# Fill missing values
df.fillna(df.median(numeric_only=True), inplace=True)
df.fillna(df.mode().iloc[0], inplace=True)

# Feature Engineering
df['like_follower_ratio'] = df['total_likes'] / df['followers']
df['post_follower_ratio'] = df['posts'] / df['followers']
df['avg_likes_ratio'] = df['avg_likes'] / df['followers']

# Encode country
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
df['country_encoded'] = encoder.fit_transform(df['country'])

# Define target and features
X = df[['followers', 'avg_likes', '60_day_eng_rate', 'new_post_avg_like', 'like_follower_ratio']]
y = df['influence_score']

print("Data Preprocessing Completed!")

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Random Forest with Hyperparameter Tuning
param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
}
rf_grid = GridSearchCV(RandomForestRegressor(), param_grid_rf, cv=3, scoring='r2')
rf_grid.fit(X_train_scaled, y_train)
best_rf = rf_grid.best_estimator_

# Gradient Boosting Model with Hyperparameter Tuning
param_grid_gb = {
    'n_estimators': [100, 200],
    'learning_rate': [0.05, 0.1],
    'max_depth': [3, 5],
}
gb_grid = GridSearchCV(GradientBoostingRegressor(), param_grid_gb, cv=3, scoring='r2')
gb_grid.fit(X_train_scaled, y_train)
best_gb = gb_grid.best_estimator_

# Evaluate Models
models = {'Random Forest': best_rf, 'Gradient Boosting': best_gb}
for name, model in models.items():
    y_pred = model.predict(X_test_scaled)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"{name} - MSE: {mse:.4f}, R2 Score: {r2:.4f}")

import joblib

# Save the trained model
joblib.dump(best_rf, "best_model.pkl")

# Save the scaler
joblib.dump(scaler, "scaler.pkl")

print("Model and Scaler saved successfully!")

import os

file_path = "best_model.pkl"

if os.path.exists(file_path):
    print("Model file exists!")
else:
    print("Model file NOT found. Train and save the model first.")

import os
print("Current Directory:", os.getcwd())

os.chdir("C:/Users/91951/PycharmProjects/Unified_Projects/")

print("Current Directory:", os.getcwd())

